{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "research_MovieBowl_TextCNN_skf_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoMurase/movie_research_2/blob/master/research_MovieBowl_TextCNN_skf_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPFCW2dg2YNZ",
        "outputId": "2791d614-772b-4318-a9f0-1353b5bfc380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJJc1x61Ault",
        "outputId": "203d6e0b-8a37-4001-dd4a-fbe96935688d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install japanize-matplotlib\n",
        "!apt-get -q -y install swig \n",
        "!apt-get install mecab\n",
        "!apt-get install libmecab-dev\n",
        "!apt-get install mecab-ipadic-utf8\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "#日本語フォントのインストール　（wordcloudなどで可視化するため）\n",
        "!apt-get -y install fonts-ipafont-gothic\n",
        "\n",
        "!pip install neologdn \n",
        "#半角を全角に変換\n",
        "!pip install mojimoji"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting japanize-matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/aa/3b24d54bd02e25d63c8f23bb316694e1aad7ffdc07ba296e7c9be2f6837d/japanize-matplotlib-1.1.2.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from japanize-matplotlib) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->japanize-matplotlib) (1.15.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.2-cp36-none-any.whl size=4120193 sha256=f314f51493720e3847471427367ad4a2f9d34913fe72c7f67a29cf324c6b7ae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/f9/fc/bc052ce743a03f94ccc7fda73d1d389ce98216c6ffaaf65afc\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.2\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (1,277 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144618 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab2 mecab mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "0 upgraded, 5 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 16.5 MB of archives.\n",
            "After this operation, 219 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Fetched 16.5 MB in 1s (11.1 MB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 145409 files and directories currently installed.)\n",
            "Preparing to unpack .../libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/juman-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 308 kB of archives.\n",
            "After this operation, 3,132 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Fetched 308 kB in 1s (527 kB/s)\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "(Reading database ... 145534 files and directories currently installed.)\n",
            "Preparing to unpack .../libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  mecab-ipadic\n",
            "The following NEW packages will be installed:\n",
            "  mecab-ipadic mecab-ipadic-utf8\n",
            "0 upgraded, 2 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 12.1 MB of archives.\n",
            "After this operation, 54.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 12.1 MB in 1s (9,470 kB/s)\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "(Reading database ... 145542 files and directories currently installed.)\n",
            "Preparing to unpack .../mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/06/2aeff86243c88580ccf78b136d403ce5e0a1eed9091103157f01e806499f/mecab_python3-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.1\n",
            "Collecting unidic-lite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/d2/a4233f65f718f27065a4cf23a2c4f05d8bd4c75821e092060c4efaf28e66/unidic-lite-1.0.7.tar.gz (47.3MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3MB 64kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.7-cp36-none-any.whl size=47556594 sha256=ac9e26243f8ebaaa3eebda69113282ae20f8cca768bb7ef77263b1ecd2feb480\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/82/7d/086724645e33a575aafd0b1dae2835c37d2c00c6a0a96ee3a0\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.7\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-ipafont-mincho\n",
            "The following NEW packages will be installed:\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "0 upgraded, 2 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 8,251 kB of archives.\n",
            "After this operation, 28.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-ipafont-gothic all 00303-18ubuntu1 [3,526 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-ipafont-mincho all 00303-18ubuntu1 [4,725 kB]\n",
            "Fetched 8,251 kB in 1s (7,567 kB/s)\n",
            "Selecting previously unselected package fonts-ipafont-gothic.\n",
            "(Reading database ... 145587 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-ipafont-gothic_00303-18ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-gothic (00303-18ubuntu1) ...\n",
            "Selecting previously unselected package fonts-ipafont-mincho.\n",
            "Preparing to unpack .../fonts-ipafont-mincho_00303-18ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-mincho (00303-18ubuntu1) ...\n",
            "Setting up fonts-ipafont-gothic (00303-18ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\n",
            "Setting up fonts-ipafont-mincho (00303-18ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-mincho/ipam.ttf to provide /usr/share/fonts/truetype/fonts-japanese-mincho.ttf (fonts-japanese-mincho.ttf) in auto mode\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Collecting neologdn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/fd/9e84b382e4f12b73737faabeeb57fd617198dbb29b7084e28604803f7926/neologdn-0.4.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: neologdn\n",
            "  Building wheel for neologdn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neologdn: filename=neologdn-0.4-cp36-cp36m-linux_x86_64.whl size=186923 sha256=708b5f5ff01433585e7096022a04c06c5835c7bab41c15c8eff1e0b9eacd1526\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/6f/d4/c132b4e7aef22019e307e7673d97010644c9c15f28c0d0b018\n",
            "Successfully built neologdn\n",
            "Installing collected packages: neologdn\n",
            "Successfully installed neologdn-0.4\n",
            "Collecting mojimoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/0e/eb8297652315519ccc0ca3da9e06f0457d87e27f1000f696ca537914856f/mojimoji-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: mojimoji\n",
            "Successfully installed mojimoji-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJFwBqH6xV4t"
      },
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9D4zQ1v4mT5",
        "outputId": "293b53fc-2fcf-4afc-85c8-8e84f26fec49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "torch.manual_seed(1)\n",
        "random_state = 42\n",
        "\n",
        "import gc \n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "#評価\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "#混同行列\n",
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline  \n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHyY5wI1FflX"
      },
      "source": [
        "MODEL_NAME = 'TextCNN_MovieBowl_0.4'\n",
        "MODELS_DIR = '/content/drive/My Drive/movie_research/models/'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdbxv7zA9Hh"
      },
      "source": [
        "path = \"/content/drive/My Drive/movie_research/review_csv/SF_data.csv\"\n",
        "#data = pd.read_csv(path) \n",
        "\n",
        "#df = data[data[\"title\"] == \"レディ・プレイヤー1|Ready Player One\"]\n",
        "#del data\n",
        "#gc.collect()\n",
        "#print('{}件のデータを扱う'.format(len(df)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVHj_Gb0BGzk"
      },
      "source": [
        "#数値カラムに変換する\n",
        "def prepro(df, col):\n",
        "  df = df.copy() \n",
        "  df[col] = df[col].replace('-','-1') #回答してない人を0埋め\n",
        "  df[col] = df[col].astype(float)\n",
        "\n",
        "  return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAOqc2vICVTl"
      },
      "source": [
        "import neologdn \n",
        "import string \n",
        "import mojimoji \n",
        "import re \n",
        "\n",
        "def text_preprocess(text):\n",
        "\n",
        "  text = neologdn.normalize(text)\n",
        "  #URLを除去する\n",
        "  text = re.sub(\n",
        "      r'(http|https)://([-\\w]+\\.)+[-\\w]+(/[-\\w./?%&=]*)?', \n",
        "      \"\",\n",
        "      text)\n",
        "  #全角から半角に\n",
        "  text = mojimoji.zen_to_han(text) \n",
        "\n",
        "  #3D,2Dを残したい\n",
        "  text = text.replace('3D','三次元')\n",
        "  text = text.replace('2D','二次元')\n",
        "\n",
        "  #数字をすべて0に置換 \n",
        "  text = re.sub(r'\\d+', '0', text)\n",
        "\n",
        "  #string.punctuationの除去\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  text = text.translate(table) \n",
        "\n",
        "  text = text.replace(\"｡\",'')\n",
        "  text = text.replace(\"。\",'')\n",
        "  text = text.replace(\"，\",'')\n",
        "  text = text.replace(\",\",'')\n",
        "  text = text.replace(\"、\",'')\n",
        "  text = text.replace(\"､\",'')\n",
        "  text = text.replace(\"…\",'')\n",
        "  text = text.replace(\"･\",'')\n",
        "  \n",
        "\n",
        "  # 【】の除去\n",
        "  text = re.sub(r'[【】]', '', text)\n",
        "  # （）の除去\n",
        "  text = re.sub(r'[（）()]', '', text)\n",
        "  # ［］の除去\n",
        "  text = re.sub(r'[［］\\[\\]]', '', text)\n",
        "  #アルファベットをの大文字を小文字に変換\n",
        "  text = text.lower()\n",
        "\n",
        "  #改行コードの除去\n",
        "  text = text.replace('\\n', '')\n",
        "  text = text.replace('\\r\\n','')\n",
        "  \n",
        "  #空白を除去\n",
        "  #全角スペース\n",
        "  text = text.replace('\\u3000','')\n",
        "  #タブキー\n",
        "  text = text.replace('\\t','')\n",
        "  #半角\n",
        "  #text = re.sub(r' ','', text) →英字幕などの指摘もあるため後で行う\n",
        "  \n",
        "  return text "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMZmlYpoCW_X",
        "outputId": "8e57d7fa-837c-4505-e6fa-2c69ae3f8317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "!pip install emoji \n",
        "!pip install nagisa \n",
        "#!pip install unicodedata \n",
        "\n",
        "import emoji\n",
        "import nagisa\n",
        "\n",
        "def delete_emoji(text):\n",
        "  target_list = [w for w in text if w in emoji.UNICODE_EMOJI]\n",
        "  for trg in target_list:\n",
        "    text = text.replace(trg, \"\")\n",
        "  return text "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
            "\r\u001b[K     |██████▍                         | 10kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=fe020e943a347105fa51249a407dffa2cbaea5837c6b4aa403560cedeac74ac0\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n",
            "Collecting nagisa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/c2/f520a4e4ae9c0abf70d5f94149df49f06bcf78fb3a9dd57f57806868129f/nagisa-0.2.7-cp36-cp36m-manylinux1_x86_64.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 140kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nagisa) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nagisa) (1.15.0)\n",
            "Collecting DyNet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/f0/01a561a301a8ea9aea1c28f82e108c38cd103964c7a46286ab01757a4092/dyNET-2.1-cp36-cp36m-manylinux1_x86_64.whl (28.1MB)\n",
            "\u001b[K     |████████████████████████████████| 28.1MB 110kB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from DyNet->nagisa) (0.29.21)\n",
            "Installing collected packages: DyNet, nagisa\n",
            "Successfully installed DyNet-2.1 nagisa-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T2sFujaCfNq"
      },
      "source": [
        "import MeCab\n",
        "def mecab_dokuritugo(text):\n",
        "  tagger = MeCab.Tagger() \n",
        "  tagger.parse('')\n",
        "  node = tagger.parseToNode(text) \n",
        "  word_list = [] \n",
        "  while node: \n",
        "    pos = node.feature.split(\",\")[0] ### 単語の品詞を抽出\n",
        "    if pos in [\"動詞\",\"形容詞\"]:\n",
        "      ### 動詞,形容詞の原型を抽出 \n",
        "      ### [6]は原型がカタカナになって返ってくる\n",
        "      word = node.feature.split(\",\")[7]  \n",
        "      word_list.append(word) \n",
        "\n",
        "    elif pos in [\"名詞\"]: ### 名詞はそのまま\n",
        "      word = node.surface \n",
        "      word_list.append(word)\n",
        "    node = node.next\n",
        "  return \" \".join(word_list)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZhwhTcp-_gV"
      },
      "source": [
        "class TextCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, class_num, kernel_num, kernel_sizes, dropout, static):\n",
        "    \"\"\"\n",
        "     :param vocab_size: int, 入力言語の語彙数\n",
        "     :param embedding_size: int, 埋め込みベクトルの次元数\n",
        "     :param class_num: int, 出力のクラス数\n",
        "     :param kernel_num: int,　畳み込み層の出力チャネル数\n",
        "     :param kernel_sizes: list of int, カーネルのウィンドウサイズ\n",
        "     :param dropout: float, ドロップアウト率\n",
        "     :param static: bool, 埋め込みを固定するか否かのフラグ\n",
        "    \n",
        "    \"\"\"\n",
        "    super(TextCNN, self).__init__()\n",
        "    self.static = static \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "    # nn.ModuleList: 任意の数のModuleをlistのような形で保持することができるクラス\n",
        "    self.convs = nn.ModuleList(\n",
        "        [nn.Conv1d(1, kernel_num, (kernel_size, embedding_size)) for kernel_size in kernel_sizes]\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout) \n",
        "    self.out = nn.Linear(len(kernel_sizes)*kernel_num, class_num)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (batch_size, max_length) \n",
        "    x = self.embedding(x) #(batch_size, max_length, embedding_size)\n",
        "\n",
        "    if self.static:\n",
        "      x = torch.tensor(x) #埋め込みを固定\n",
        "\n",
        "    # (batch_size, 1, max_length, embedding_size)\n",
        "    x = x.unsqueeze(1) \n",
        "    # [(batch_size, kernel_num, max_length-kernel_size+1), ...]*len(kernel_sizes)\n",
        "    x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] \n",
        "    # [(batch_size, kernel_num), ...]*len(kernel_sizes)\n",
        "    x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
        "    # [(batch_size, len(kernel_sizes)*kernel_num)]\n",
        "    x = torch.cat(x, 1)\n",
        "    x = self.dropout(x) \n",
        "\n",
        "    logit = self.out(x) \n",
        "    return logit "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZmqpL3t-hR"
      },
      "source": [
        "def sentence_to_ids(vocab, sen):\n",
        "  \"\"\"\n",
        "  単語のリストをIDのリストに変換する関数\n",
        "  :param vocab: class Vocab object\n",
        "  :param sen : list of str, 文を分かち書きして得られた単語のリスト\n",
        "  \"\"\"\n",
        "  UNK = 1\n",
        "\n",
        "  #辞書にない言葉にUNKを割り当てる\n",
        "  out = [vocab.word2id.get(word, UNK) for word in sen ]\n",
        "  return out \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY_4SHvHKhdt"
      },
      "source": [
        "class Vocab(object):\n",
        "  def __init__(self, word2id={}):\n",
        "    self.word2id = dict(word2id)\n",
        "    self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "  \n",
        "  def build_vocab(self, sentences, min_count=1):\n",
        "    \"\"\"\n",
        "    コーパスから語彙の辞書を構築するメソッド\n",
        "\n",
        "    :param sentences: list of list of str, コーパス\n",
        "    :param min_count: int, 辞書に含める単語の最小出現回数\n",
        "    \"\"\"\n",
        "    word_counter = {}\n",
        "    for sentence in sentences:\n",
        "      for word in sentence:\n",
        "        # dict.get(key, 0)はdictにdict[key],なければ0を返す\n",
        "        word_counter[word] = word_counter.get(word, 0) + 1\n",
        "    \n",
        "    #min_count回以上出現する単語を加える\n",
        "    for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
        "      if count < min_count : \n",
        "        break \n",
        "      _id = len(self.word2id)\n",
        "      self.word2id.setdefault(word, _id)\n",
        "      self.id2word[_id] = word "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ddEB2LOr-hf"
      },
      "source": [
        "def make_dataset(df):\n",
        "  # 特殊なトークンは事前に定義しておく\n",
        "  PAD = 0\n",
        "  UNK = 1\n",
        "  PAD_TOKEN = '<PAD>'\n",
        "  UNK_TOKEN = '<UNK>'\n",
        "\n",
        "  MIN_COUNT = 1  # 語彙に含める単語の最低出現回数\n",
        "\n",
        "  # 単語をIDに変換する辞書の初期値を設定\n",
        "  word2id = {\n",
        "      PAD_TOKEN: PAD,\n",
        "      UNK_TOKEN: UNK,\n",
        "      }\n",
        "  vocab = Vocab(word2id=word2id)\n",
        "  vocab.build_vocab(df.values.tolist(), min_count=MIN_COUNT)\n",
        "\n",
        "  print(\"語彙数:\", len(vocab.word2id))\n",
        "  \n",
        "  return vocab"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MDagsMA8bZ9"
      },
      "source": [
        "def train_function(dataloader, model, criterion, optimizer, scheduler, DEVICE, epoch):\n",
        "  model.train() \n",
        "  total_loss = 0 \n",
        "  total_corrects = 0\n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "\n",
        "  #progress = tqdm(dataloader, total=len(dataloader))\n",
        "\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    #progress.set_description(f\"<Train> Epoch{epoch+1}\")\n",
        "    batch_X, batch_Y = batch \n",
        "    del batch\n",
        "    batch_X, batch_Y = Variable(batch_X), Variable(batch_Y),\n",
        "    optimizer.zero_grad() \n",
        "\n",
        "    outputs = model(batch_X).float()\n",
        "    del batch_X \n",
        "    loss = criterion(outputs, batch_Y)\n",
        "    loss.backward()\n",
        "    optimizer.step() \n",
        "    scheduler.step() \n",
        "    preds = torch.max(outputs, 1)[1].view(batch_Y.size())\n",
        "    del outputs \n",
        "\n",
        "    total_loss += loss.item() \n",
        "    del loss \n",
        "    total_corrects += torch.sum(preds == batch_Y)\n",
        "\n",
        "    all_labels += batch_Y.tolist() \n",
        "    all_preds += preds.tolist() \n",
        "    del batch_Y, preds \n",
        "\n",
        "    #progress.set_postfix(loss=total_loss/(i+1), f1=f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    gc.collect()\n",
        "  train_loss = total_loss / len(dataloader)\n",
        "  #train_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
        "  train_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "  accuracy = accuracy_score(all_labels, all_preds)\n",
        "  #tp / (tp+fn)\n",
        "  #recall = recall_score(all_labels, all_preds)\n",
        "  #tp / (tp+fp)\n",
        "  #precision = precision_score(all_labels, all_preds)\n",
        "  #F1=2*(precision*recall)/(precision + recall)\n",
        "  #f1_score = f1_scorey(y_true, y_pred)\n",
        "  gc.collect()\n",
        "  return train_loss, accuracy, train_f1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27AO5FOf8bQa"
      },
      "source": [
        "def evalu_function(dataloader, model, criterion, device, epoch):\n",
        "  model.eval() \n",
        "  total_loss = 0\n",
        "  total_corrects = 0 \n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "\n",
        "  with torch.no_grad(): \n",
        "    #progress = tqdm(dataloader, total=len(dataloader))\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "      #progress.set_description(f\"Valid - Epoch{epoch+1}\")\n",
        "      batch_X, batch_Y = batch \n",
        "      del batch\n",
        "      batch_X, batch_Y = Variable(batch_X), Variable(batch_Y),    \n",
        "      \n",
        "      outputs = model(batch_X).float()\n",
        "      del batch_X \n",
        "      loss = criterion(outputs, batch_Y)\n",
        "      preds = torch.max(outputs, 1)[1].view(batch_Y.size())\n",
        "      del outputs \n",
        "\n",
        "\n",
        "      total_loss += loss.item() \n",
        "      del loss \n",
        "      total_corrects += torch.sum(preds == batch_Y)\n",
        "\n",
        "      all_labels += batch_Y.tolist() \n",
        "      all_preds += preds.tolist() \n",
        "      del batch_Y, preds \n",
        "\n",
        "      #progress.set_postfix(loss=total_loss/(i+1), f1=f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "      gc.collect()\n",
        "\n",
        "    valid_loss = total_loss / len(dataloader)\n",
        "    #train_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\n",
        "    #valid_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    #tp / (tp+fn)\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    #tp / (tp+fp)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    #valid_f1=2*(precision*recall)/(precision + recall)\n",
        "    valid_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    gc.collect()\n",
        "\n",
        "    return valid_loss, accuracy, recall, precision, valid_f1  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcdJiLbF8FPV"
      },
      "source": [
        "def plot_training(train_losses, train_accs, train_f1s,\n",
        "                  valid_losses, valid_accs, \n",
        "                  valid_recalls,valid_precisions,\n",
        "                  valid_f1s,\n",
        "                  epoch, fold):\n",
        "    \n",
        "    loss_df = pd.DataFrame({\"Train\":train_losses,\n",
        "                            \"Valid\":valid_losses},\n",
        "                        index=range(1, epoch+2))\n",
        "    loss_df.to_csv(f\"/content/drive/My Drive/movie_research/result/loss_plot_fold={fold}.csv\", index=False)\n",
        "    loss_ax = sns.lineplot(data=loss_df).get_figure()\n",
        "    #/content/drive/My Drive/movie_research\n",
        "    loss_ax.savefig(f\"/content/drive/My Drive/movie_research/figures/loss_plot_fold={fold}.png\", dpi=300)\n",
        "    loss_ax.clf()\n",
        "\n",
        "    acc_df = pd.DataFrame({\"Train\":train_accs,\n",
        "                           \"Valid\":valid_accs},\n",
        "                          index=range(1, epoch+2))\n",
        "    acc_df.to_csv(f\"/content/drive/My Drive/movie_research/result/acc_plot_fold={fold}.csv\", index=False)\n",
        "    acc_ax = sns.lineplot(data=acc_df).get_figure()\n",
        "    acc_ax.savefig(f\"/content/drive/My Drive/movie_research/figures/acc_plot_fold={fold}.png\", dpi=300)\n",
        "    acc_ax.clf()\n",
        "\n",
        "    f1_df = pd.DataFrame({\"Train_f1\":train_f1s,\n",
        "                          \"Valid_f1\":valid_f1s,\n",
        "                          \"Valid_precision\":valid_precisions,\n",
        "                          \"Valid_recall\":valid_recalls,\n",
        "                            },\n",
        "                         index=range(1, epoch+2))\n",
        "    f1_df.to_csv(f\"/content/drive/My Drive/movie_research/result/f1_plot_fold={fold}.csv\", index=False)\n",
        "    f1_ax = sns.lineplot(data=f1_df).get_figure()\n",
        "    f1_ax.savefig(f\"/content/drive/My Drive/movie_research/figures/f1_plot_fold={fold}.png\", dpi=300)\n",
        "    f1_ax.clf()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFI5pRhhLzGh"
      },
      "source": [
        "class DataLoader(object):\n",
        "  def __init__(self, X, Y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    :param X: list, 入力言語の文章のリスト (単語IDのリスト)\n",
        "    :param Y: list, 出力言語の文章のリスト (単語IDのリスト)\n",
        "    :param batch_size: int, バッチサイズ\n",
        "    :param shuffle: bool, サンプルをシャッフルするかどうか\n",
        "    \"\"\"\n",
        "    self.data = list(zip(X, Y))\n",
        "    self.batch_size = batch_size \n",
        "    self.shuffle = shuffle \n",
        "    self.start_index = 0 \n",
        "    self.reset() \n",
        "\n",
        "  def reset(self):\n",
        "    if self.shuffle:\n",
        "      self.data = shuffle(self.data, random_state=random_state)\n",
        "    self.start_index = 0 \n",
        "\n",
        "  def __iter__(self):\n",
        "    return self \n",
        "  def __len__(self) -> int:\n",
        "    return len(self.data)\n",
        "  def __next__(self):\n",
        "\n",
        "  #ポインタが最後まで到達したら初期化する\n",
        "    if self.start_index >= len(self.data): \n",
        "      self.reset() \n",
        "      raise StopIteration() \n",
        "\n",
        "    #バッチを取得\n",
        "    X, Y = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n",
        "    #短い系列の末尾をパディングする\n",
        "    lengths_X = [len(s) for s in X]\n",
        "    max_length_X = max(lengths_X) \n",
        "    padded_X = [self.pad_seq(s, max_length_X) for s in X] \n",
        "    #tensorに変換\n",
        "    batch_X = torch.tensor(padded_X, dtype=torch.long, device=device)\n",
        "    batch_Y = torch.tensor(Y, dtype=torch.long, device=device)\n",
        "\n",
        "    #ポインタを更新する\n",
        "    self.start_index += self.batch_size \n",
        "\n",
        "    return batch_X, batch_Y \n",
        "  \n",
        "  @staticmethod \n",
        "  def pad_seq(seq, max_length):\n",
        "    \"\"\"\n",
        "    系列の末尾をパディングする \n",
        "    :param seq: list or int, 単語のインデックスのリスト\n",
        "    :param seq: max_length: int, バッチ内の系列の最大長\n",
        "    :return seq: list of int, 単語のインデックスのリスト\n",
        "    \"\"\"\n",
        "    PAD = 0\n",
        "    seq += [PAD for i in range(max_length - len(seq))]\n",
        "    return seq "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC2wwcYD7U6J"
      },
      "source": [
        "def trainer(fold, df, batch_size, lr, EPOCHS, vocab):\n",
        "\n",
        "  train_df = df[df.kfold != fold].reset_index(drop=True)\n",
        "  valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  train_X = train_df[\"tokenize\"]\n",
        "  train_y = train_df[\"score\"]\n",
        "  valid_X = valid_df[\"tokenize\"]\n",
        "  valid_y = valid_df[\"score\"] \n",
        "\n",
        "  #訓練データを使って辞書を作る\n",
        "  #vocab = make_dataset(train_X)\n",
        "\n",
        "  ###データセットの作成\n",
        "  #各データをid化\n",
        "  train_X = [sentence_to_ids(vocab, x) for x in train_X]\n",
        "  valid_X = [sentence_to_ids(vocab, x) for x in valid_X]\n",
        "\n",
        "  train_dataloader = DataLoader(train_X, train_y, batch_size)\n",
        "  valid_dataloader = DataLoader(valid_X, valid_y, batch_size)\n",
        "\n",
        "  model_args = {\n",
        "      'vocab_size': len(vocab.id2word),\n",
        "      'embedding_size': 128,\n",
        "      'class_num': 9,\n",
        "      'kernel_num': 64, \n",
        "      'kernel_sizes': [3,4,5],\n",
        "      'dropout': 0.3, \n",
        "      'static': False,\n",
        "  }\n",
        "  #modelの設定 \n",
        "  model = TextCNN(**model_args)\n",
        "  model = model.to(device)\n",
        "\n",
        "  #if weight != None:\n",
        "  #  criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "  #else:\n",
        "  #  criterion = nn.CrossEntropyLoss()\n",
        "  #criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  #weightの計算: scoreの分布にばらつきがあるため\n",
        "  weight = 1 / pd.DataFrame(train_y).reset_index().groupby(\"score\").count().values\n",
        "  weight /= weight.sum()\n",
        "  weights = torch.FloatTensor(weight).to(device)\n",
        "  del weight\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "  #AdamWを使ってみる\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=1.0)\n",
        "\n",
        "  train_losses = []\n",
        "  train_accs = []\n",
        "  train_f1s = []\n",
        "  valid_losses = []\n",
        "  valid_accs = []\n",
        "  valid_f1s = []\n",
        "  valid_recalls = []\n",
        "  valid_precisions = []\n",
        " \n",
        "  best_loss = np.inf \n",
        "  best_acc = 0 \n",
        "  best_f1 = 0 \n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc, train_f1 = train_function(train_dataloader, model, criterion, optimizer, scheduler, device, epoch)\n",
        "    valid_loss, valid_acc, valid_recall, valid_precision, valid_f1 = evalu_function(valid_dataloader, model, criterion, device, epoch)\n",
        "    print(\"Train:\\n\")\n",
        "    print(f\"Loss: {train_loss}  Acc: {train_acc}  f1: {train_f1}  \")\n",
        "    print(\"Valid:\\n\")\n",
        "    print(f\"Loss: {valid_loss}  Acc: {valid_acc}  f1: {valid_f1}  \")\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    train_f1s.append(train_f1)\n",
        "    valid_losses.append(valid_loss)\n",
        "    valid_accs.append(valid_acc)\n",
        "    valid_recalls.append(valid_recall)\n",
        "    valid_precisions.append(valid_precision)\n",
        "    valid_f1s.append(valid_f1)\n",
        "\n",
        "    plot_training(train_losses, train_accs, train_f1s,\n",
        "                      valid_losses, valid_accs, \n",
        "                      valid_recalls,valid_precisions,\n",
        "                      valid_f1s,\n",
        "                      epoch, fold)\n",
        "\n",
        "    best_loss = valid_loss if valid_loss < best_loss else best_loss\n",
        "    besl_acc = valid_acc if valid_acc > best_acc else best_acc\n",
        "    if valid_f1 > best_f1:\n",
        "      best_f1 = valid_f1\n",
        "      print(\"model saving!\", end=\"\")\n",
        "      torch.save(model.state_dict(), MODELS_DIR + f\"best_{MODEL_NAME}_{fold}.pth\")\n",
        "      print(\"\\n\")\n",
        "\n",
        "    return best_f1\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHdvZHR-9yO7"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def create_folds(df, num_splits):\n",
        "    df = df.copy()\n",
        "    #df[\"jobflag\"] = df[\"jobflag\"] - 1\n",
        "    df[\"kfold\"] = -1 \n",
        "\n",
        "    #fracで抽出する行の割合1は100%\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    y = df[\"score\"].values \n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "    \n",
        "    skf = StratifiedKFold(n_splits=num_splits)\n",
        "\n",
        "    for f, (t_, v_) in enumerate(skf.split(X=df, y=y)):\n",
        "        df.loc[v_,'kfold'] = f ###locより高速\n",
        "    return df"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xj9fLdy1G44"
      },
      "source": [
        "def pipeline(path):\n",
        "  data = pd.read_csv(path)\n",
        "  data = data.sample(frac=0.4).reset_index(drop=True)\n",
        "  #data = data[data[\"title\"] == \"レディ・プレイヤー1|Ready Player One\"]\n",
        "  #重複の削除\n",
        "  data = data.drop_duplicates(keep='first') \n",
        "\n",
        "  #数値カラムに変換\n",
        "  data = prepro(data, \"score\") \n",
        "\n",
        "  #0.5刻みにしたスコアの作成\n",
        "  data[\"score2\"] = data[\"score\"]*2.0\n",
        "  data[\"score2\"] = data[\"score2\"].round() / 2\n",
        "\n",
        "  #preprocessing to text\n",
        "  data[\"review\"] = data[\"review\"].apply(text_preprocess) \n",
        "  data[\"review\"] = data[\"review\"].apply(delete_emoji)\n",
        "  #df[\"review\"] = df[\"review\"].apply(extra_preprocess)\n",
        "\n",
        "  data[\"length\"] = data[\"review\"].apply(lambda x: len(x))\n",
        "  #ノーコメントの削除\n",
        "  data = data[data[\"length\"]>1]\n",
        "\n",
        "  #独立語のみの抽出\n",
        "  data[\"review_tokenize\"] = data[\"review\"].apply(mecab_dokuritugo)\n",
        "  data[\"length2\"] = data[\"review_tokenize\"].apply(lambda x: len(x))\n",
        "  #前処理した結果空白になった行の削除\n",
        "  data = data[data[\"length2\"]>1]\n",
        "  gc.collect()\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-z5FAcN1EZU"
      },
      "source": [
        "df = pipeline(path)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5a3sB_0MElp",
        "outputId": "47b550c6-8d67-4c3d-db74-e3d9f1e2d62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "df[\"score2\"].value_counts().sort_values().plot(kind='bar')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9aa394c518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5Uz-q9BN2vU",
        "outputId": "dcbab53b-6c1d-4d64-be4d-1f65a719cdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "df[\"score2\"].value_counts()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0    58230\n",
              " 3.5    46678\n",
              " 4.5    23801\n",
              " 3.0    22843\n",
              " 5.0    15292\n",
              "-1.0    11394\n",
              " 2.5     6607\n",
              " 2.0     3730\n",
              " 1.5     1190\n",
              " 1.0     1127\n",
              "Name: score2, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6HNYJwe_WZ6"
      },
      "source": [
        "NUM_SPLITS = 5\n",
        "\n",
        "test_df = df[df.score == -1]\n",
        "df = df[df.score != -1]\n",
        "df = create_folds(df, NUM_SPLITS)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDRrDuLvJlRq",
        "outputId": "6d2a015e-83f5-4050-d0bc-d01c1cac7f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "df = df[[\"review_tokenize\", \"score2\",\"kfold\"]]\n",
        "df = df.rename(columns={\"review_tokenize\":\"tokenize\"})\n",
        "df"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokenize</th>\n",
              "      <th>score2</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>期待 大きい 今作 面白い 旧作 越える 恐竜 ｼｰﾝ 多い 辛い w 次回 面白い 予感 無い</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>最近 見る 中 0 番 映画 最初 有る 家族 もの 映画 思う 居る 宇宙 旅立つ ｶﾞﾗ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>自宅 ﾃﾚﾋﾞ 友人 0 鑑賞 中年 ｱﾙｺｰﾙ 中毒 ｹﾞｲﾘｰｻｲﾓﾝﾍﾟｯｸﾞ ﾐｯｼ...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000 観賞 自分 生む 子 例える 結婚 為る 相手 違う 為る 子 自分 ところ 生まれ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>予備 知識 なし 鑑賞 途中 ｴｲﾘｱﾝ 似る 生物 思う 居る 名作 ｴｲﾘｱﾝ 繋がる ...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179493</th>\n",
              "      <td>冒頭 ｼﾞｪｯﾄ 機 止める ｼｰﾝ 印象 模型 ｼｰﾝ 毎回 ﾜｸﾜｸ 為る 仕舞う</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179494</th>\n",
              "      <td>富士 山 理由 最後 舞台 成る びっくり ﾄﾞﾗﾏ 少ない ﾎﾝ</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179495</th>\n",
              "      <td>期待 為る 過ぎる ただ 選曲 ｾﾝｽ 良い ｷｬｽﾄ 豪華 素晴らしい 知る 顔 ｰ ｰ ...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179496</th>\n",
              "      <td>世界 愛する sf 映画 今頃 成る 見る e t 見る 目 怖い 少年 出会い 怖い 成る...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179497</th>\n",
              "      <td>出る 来る 宇宙 みんな 面白い 憎たらしい ﾜｰﾑ 0 番</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>179498 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tokenize  score2  kfold\n",
              "0        期待 大きい 今作 面白い 旧作 越える 恐竜 ｼｰﾝ 多い 辛い w 次回 面白い 予感 無い     3.5      0\n",
              "1       最近 見る 中 0 番 映画 最初 有る 家族 もの 映画 思う 居る 宇宙 旅立つ ｶﾞﾗ...     4.5      0\n",
              "2       自宅 ﾃﾚﾋﾞ 友人 0 鑑賞 中年 ｱﾙｺｰﾙ 中毒 ｹﾞｲﾘｰｻｲﾓﾝﾍﾟｯｸﾞ ﾐｯｼ...     3.5      0\n",
              "3       000 観賞 自分 生む 子 例える 結婚 為る 相手 違う 為る 子 自分 ところ 生まれ...     5.0      0\n",
              "4       予備 知識 なし 鑑賞 途中 ｴｲﾘｱﾝ 似る 生物 思う 居る 名作 ｴｲﾘｱﾝ 繋がる ...     3.5      0\n",
              "...                                                   ...     ...    ...\n",
              "179493        冒頭 ｼﾞｪｯﾄ 機 止める ｼｰﾝ 印象 模型 ｼｰﾝ 毎回 ﾜｸﾜｸ 為る 仕舞う     4.5      4\n",
              "179494                  富士 山 理由 最後 舞台 成る びっくり ﾄﾞﾗﾏ 少ない ﾎﾝ     3.0      4\n",
              "179495  期待 為る 過ぎる ただ 選曲 ｾﾝｽ 良い ｷｬｽﾄ 豪華 素晴らしい 知る 顔 ｰ ｰ ...     3.5      4\n",
              "179496  世界 愛する sf 映画 今頃 成る 見る e t 見る 目 怖い 少年 出会い 怖い 成る...     4.0      4\n",
              "179497                     出る 来る 宇宙 みんな 面白い 憎たらしい ﾜｰﾑ 0 番     4.0      4\n",
              "\n",
              "[179498 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIw7tCLR-mgq",
        "outputId": "25d263c7-0543-47dc-de1b-8ff2acd8ce75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "df = df.rename(columns={\"score2\":\"score\"})\n",
        "df"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokenize</th>\n",
              "      <th>score</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>期待 大きい 今作 面白い 旧作 越える 恐竜 ｼｰﾝ 多い 辛い w 次回 面白い 予感 無い</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>最近 見る 中 0 番 映画 最初 有る 家族 もの 映画 思う 居る 宇宙 旅立つ ｶﾞﾗ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>自宅 ﾃﾚﾋﾞ 友人 0 鑑賞 中年 ｱﾙｺｰﾙ 中毒 ｹﾞｲﾘｰｻｲﾓﾝﾍﾟｯｸﾞ ﾐｯｼ...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000 観賞 自分 生む 子 例える 結婚 為る 相手 違う 為る 子 自分 ところ 生まれ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>予備 知識 なし 鑑賞 途中 ｴｲﾘｱﾝ 似る 生物 思う 居る 名作 ｴｲﾘｱﾝ 繋がる ...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179493</th>\n",
              "      <td>冒頭 ｼﾞｪｯﾄ 機 止める ｼｰﾝ 印象 模型 ｼｰﾝ 毎回 ﾜｸﾜｸ 為る 仕舞う</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179494</th>\n",
              "      <td>富士 山 理由 最後 舞台 成る びっくり ﾄﾞﾗﾏ 少ない ﾎﾝ</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179495</th>\n",
              "      <td>期待 為る 過ぎる ただ 選曲 ｾﾝｽ 良い ｷｬｽﾄ 豪華 素晴らしい 知る 顔 ｰ ｰ ...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179496</th>\n",
              "      <td>世界 愛する sf 映画 今頃 成る 見る e t 見る 目 怖い 少年 出会い 怖い 成る...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179497</th>\n",
              "      <td>出る 来る 宇宙 みんな 面白い 憎たらしい ﾜｰﾑ 0 番</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>179498 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tokenize  score  kfold\n",
              "0        期待 大きい 今作 面白い 旧作 越える 恐竜 ｼｰﾝ 多い 辛い w 次回 面白い 予感 無い    3.5      0\n",
              "1       最近 見る 中 0 番 映画 最初 有る 家族 もの 映画 思う 居る 宇宙 旅立つ ｶﾞﾗ...    4.5      0\n",
              "2       自宅 ﾃﾚﾋﾞ 友人 0 鑑賞 中年 ｱﾙｺｰﾙ 中毒 ｹﾞｲﾘｰｻｲﾓﾝﾍﾟｯｸﾞ ﾐｯｼ...    3.5      0\n",
              "3       000 観賞 自分 生む 子 例える 結婚 為る 相手 違う 為る 子 自分 ところ 生まれ...    5.0      0\n",
              "4       予備 知識 なし 鑑賞 途中 ｴｲﾘｱﾝ 似る 生物 思う 居る 名作 ｴｲﾘｱﾝ 繋がる ...    3.5      0\n",
              "...                                                   ...    ...    ...\n",
              "179493        冒頭 ｼﾞｪｯﾄ 機 止める ｼｰﾝ 印象 模型 ｼｰﾝ 毎回 ﾜｸﾜｸ 為る 仕舞う    4.5      4\n",
              "179494                  富士 山 理由 最後 舞台 成る びっくり ﾄﾞﾗﾏ 少ない ﾎﾝ    3.0      4\n",
              "179495  期待 為る 過ぎる ただ 選曲 ｾﾝｽ 良い ｷｬｽﾄ 豪華 素晴らしい 知る 顔 ｰ ｰ ...    3.5      4\n",
              "179496  世界 愛する sf 映画 今頃 成る 見る e t 見る 目 怖い 少年 出会い 怖い 成る...    4.0      4\n",
              "179497                     出る 来る 宇宙 みんな 面白い 憎たらしい ﾜｰﾑ 0 番    4.0      4\n",
              "\n",
              "[179498 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp9nBU2VJkKh",
        "outputId": "5007e181-67bf-4aab-fdde-568619595c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "f1_scores = []\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.001 \n",
        "NUM_EPOCHS = 1000 \n",
        "\n",
        "vocab = make_dataset(df[\"tokenize\"])\n",
        "\n",
        "for fold in range(NUM_SPLITS):\n",
        "    print(f\"fold {fold}\", \"=\"*70)\n",
        "    f1 = trainer(fold, df,BATCH_SIZE,lr, NUM_EPOCHS,vocab)\n",
        "    f1_scores.append(f1)\n",
        "    print(f\"<fold={fold}> best score: {f1}\\n\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "語彙数: 3580\n",
            "fold 0 ======================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:\n",
            "\n",
            "Loss: 0.009301104028934998  Acc: 0.41511720218944553  f1: 0.09423348034421491  \n",
            "Valid:\n",
            "\n",
            "Loss: 0.008635506093668074  Acc: 0.44467966573816153  f1: 0.19312564160380322  \n",
            "model saving!\n",
            "\n",
            "<fold=0> best score: 0.19312564160380322\n",
            "\n",
            "fold 1 ======================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train:\n",
            "\n",
            "Loss: 0.009283294331839985  Acc: 0.4175545620412541  f1: 0.10854378003801457  \n",
            "Valid:\n",
            "\n",
            "Loss: 0.008743847772935638  Acc: 0.44061281337047353  f1: 0.1761545550165088  \n",
            "model saving!\n",
            "\n",
            "<fold=1> best score: 0.1761545550165088\n",
            "\n",
            "fold 2 ======================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d11d605ff458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_SPLITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"fold {fold}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mf1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"<fold={fold}> best score: {f1}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b6ad321ec009>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(fold, df, batch_size, lr, EPOCHS, vocab)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevalu_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-35fd7e9cf41a>\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(dataloader, model, criterion, optimizer, scheduler, DEVICE, epoch)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#progress.set_postfix(loss=total_loss/(i+1), f1=f1_score(all_labels, all_preds, average=\"macro\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m#train_acc = total_corrects.double().cpu().detach().numpy() / len(dataloader.dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4JRGWj3Z9Dk"
      },
      "source": [
        "#だいたいcv F値0.2いかないくらいだろうな"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHLc32s_mKQ"
      },
      "source": [
        "cv = sum(f1_scores) / len(f1_scores)\n",
        "print(f\"CV: {cv}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwFW2L0FIkoD"
      },
      "source": [
        "df[\"length\"] = df[\"tokenize\"].apply(lambda x: len(x))\n",
        "plt.figure(figsize=(20, 5))\n",
        "sns.scatterplot(y=df[\"score\"].sort_values(), x=df[\"length\"], palette='YlOrBr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sao_8PWMgBcv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}